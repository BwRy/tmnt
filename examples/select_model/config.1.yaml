## Optimizable hyper-parameters for TMNT topic models
## This config is mostly illustrative and not necessarily a good hyper-parameter space to define
## See config.yaml for a good starting config file
---
  lr: {range: [1e-6, 1e-2]}   ## learning rate

  batch_size: {i_range: [100,400], step: 100} ## batch size, use much larger size for larger datasets

  latent_distribution: [vmf, gaussian, logistic_gaussian]

  ## stochastic gradient-based optimizer
  optimizer: [adam, ftml]

  ## number of latent topics
  n_latent: {i_range: [20, 40], step: 5}

  enc_hidden_dim: {i_range: [50, 400], step: 50}

  ## CONDITIONAL on latent_distribution==vmf
  kappa: {range: [1.0, 100.0]}
  
  target_sparsity: {range: [0.0, 0.1]}
  
  coherence_regularizer_penalty: {range: [0.0, 0.1] }

  ## pre-trained embedding vocab + weights
  embedding_source: [glove:glove.42B.300d, glove:glove.6B.200d, fasttext:wiki.en, random]

  ## CONDITIONAL on embedding_source != random
  fixed_embedding: [True, False]  

  ## CONDITIONAL on embedding_source == random  
  embedding_size: {i_range: [50, 400], step: 50}