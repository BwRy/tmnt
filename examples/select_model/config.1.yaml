## Optimizable hyper-parameters for TMNT topic models
## This config is mostly illustrative and not necessarily a good hyper-parameter space to define
## See config.yaml for a good starting config file
---
  lr: {range: [1e-6, 1e-2]}   ## learning rate

  batch_size: {i_range: [100,400], step: 100} ## batch size, use much larger size for larger datasets

  latent_distribution: [logistic_gaussian]

  ## stochastic gradient-based optimizer
  optimizer: [adam, ftml]

  ## number of latent topics
  n_latent: {i_range: [20, 40], step: 5}

  ## dimension of each encoder layer (all layers same dimensionality)
  enc_hidden_dim: {i_range: [50, 400], step: 50}

  ## number of fully connected encoder layers
  num_enc_layers: {i_range: [1, 3], step: 1}

  ## CONDITIONAL on latent_distribution==vmf
  ## concentration parameter for von Mises-Fischer distribution
  kappa: {range: [1.0, 100.0]}

  ## CONDITIONAL on latent_distribution==logistic_gaussian
  ## prior variance for logistic_gaussian
  alpha: {range: [0.1, 10.0]}

  target_sparsity: {range: [0.0, 0.1]}
  
  coherence_loss_wt: {range: [0.0, 1.0] } ## can be more than 1.0

  redundancy_loss_wt: {range: [0.0, 1.0] } ## can be more than 1.0

  ## pre-trained embedding vocab + weights
  embedding_source: [glove:glove.42B.300d, glove:glove.6B.200d, fasttext:wiki.en, random]

  ## CONDITIONAL on embedding_source != random
  fixed_embedding: [True, False]  

  ## CONDITIONAL on embedding_source == random  
  embedding_size: {i_range: [50, 400], step: 50}

  